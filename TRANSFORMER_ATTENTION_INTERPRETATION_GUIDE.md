# Transformer Attention Interpretation Guide for Deepfake Detection

This guide explains how to interpret the attention visualizations generated by the explainability toolkit. These visualizations reveal **how the transformer model analyzes video frames to detect deepfakes**.

---

## Overview

The transformer attention mechanism shows which frames the model focuses on when making predictions. By analyzing these attention patterns, you can understand:

1. **What temporal patterns** the model uses to detect deepfakes
2. **Whether attention patterns differ** between real and fake videos
3. **Which frames are most critical** for the detection decision

---

## Visualizations Explained

### 1. **Per-Layer Attention Heatmaps** (`layer_X_all_heads.png`)

These files show attention patterns for each of the 4 transformer layers.

#### What You See:
- **Layout**: 2×4 grid of 8 heatmaps (one per attention head)
- **Axes**: 
  - X-axis (horizontal) = "Key Frame" (what the model attends to)
  - Y-axis (vertical) = "Query Frame" (the frame doing the attending)
- **Colors**: Yellow/bright = high attention, dark/purple = low attention
- **Matrix Size**: 100×100 (100 frames × 100 frames)

#### How to Interpret:

**Bright horizontal lines** at a specific query frame:
- That frame is paying strong attention to multiple key frames
- Strong temporal dependencies exist at that point in the video

**Bright vertical lines** at a specific key frame:
- Many query frames are attending to that key frame
- That frame contains important information for multiple decisions

**Diagonal pattern** (top-left to bottom-right):
- Each frame mostly attends to itself (expected behavior)
- Model relies on local temporal continuity

**Off-diagonal clusters** (bright squares away from diagonal):
- Model attends to distant frames in the sequence
- Complex temporal relationships are being captured

#### Real vs Fake Differences:

**Real Videos typically show:**
- Smooth, consistent attention patterns
- Strong self-attention (diagonal focus)
- Gradual transitions between frames
- Predictable temporal flow

**Fake Videos typically show:**
- Erratic attention patterns
- Sudden jumps in attention
- Inconsistent relationships between frames
- Unusual temporal artifacts

---

### 2. **Summary Visualization** (`transformer_attention_summary.png`)

A 2×2 grid showing aggregate analysis across all layers and heads.

#### Plot 1: Average Attention (Top-Left)
Shows the combined attention pattern across all 4 layers and 8 heads.

**What to Look For:**
- **Smooth gradient patterns**: Normal, consistent temporal evolution
- **Spiky patterns**: Anomalies or specific frames requiring special attention
- **Symmetric patterns**: Unusual - suggests artificial or engineered sequences
- **Random noise**: Model may be struggling to find patterns

**Deepfake Indicators:**
- Sudden bright spots away from the diagonal
- Compressed attention (all focus on a few frames)
- Regular periodic patterns (artificial)

---

#### Plot 2: Self-Attention Strength (Top-Right)
Shows how much each frame attends to **itself** across the video.

**Y-axis**: Self-attention weight (0 to ~1)
**X-axis**: Frame number (0 to 100)

**Interpretation:**

**High self-attention (peaks)**:
- Frame is confident or important
- Contains distinctive features
- May be a transition point or anomaly

**Low self-attention (dips)**:
- Frame is uncertain
- Heavily dependent on context
- Possibly artificially generated or blended

**Smooth curve**:
- Normal, consistent video properties
- Expected for real videos

**Erratic pattern** (sudden jumps/drops):
- Potential deepfake indicator
- Inconsistent frame generation
- Encoding artifacts

**Real Videos**: Typically smooth curve with gradual variations
**Fake Videos**: Often show spikes at certain frames (glitch points, artifacts)

---

#### Plot 3: Attention Range (Bottom-Left)
Shows the **average distance** each frame attends to.

**What it Means**:
- **Low values** (~5-10 frames): Local temporal focus (short-term dependencies)
- **High values** (~20-40 frames): Long-range temporal dependencies
- **Increasing trend**: Growing temporal awareness through video
- **Decreasing trend**: Less context needed as video progresses

**Interpretation**:

**Constant/Smooth Range**:
- Consistent temporal reasoning
- Expected for natural videos

**Spikes (red markers)**:
- Unusual temporal relationships
- Frame requires distant context
- Potential detection trigger point
- **Deepfake Indicator**: May signal artificial temporal inconsistencies

**Sudden Drops**:
- Localized anomalies
- Frame is independent of context
- Possible splice point or artifact

**Red-Marked Unusual Frames**:
- More than 2 standard deviations from mean
- Model identified as requiring special attention
- High-probability anomaly points

---

#### Plot 4: Model Prediction (Bottom-Right)
Bar chart showing the model's final decision.

**What it Shows**:
- Probability of "Real" (green)
- Probability of "Fake" (red)
- Overall confidence percentage

**Interpreting Confidence**:

| Confidence | Interpretation |
|-----------|-----------------|
| 90-100% | Very confident decision, strong patterns detected |
| 75-90% | Confident, clear evidence of real/fake properties |
| 50-75% | Moderate confidence, some ambiguous frames |
| < 50% | Low confidence, mixed or unclear signals |

**Important**: Use this alongside attention patterns to validate the decision.

---

## Comparing Real vs Fake Videos

### Real Video Attention Patterns:

✓ **Smooth, predictable heatmaps**
- Diagonal dominance with gradual fading
- No sudden attention jumps

✓ **Stable self-attention curve**
- Gradual changes
- Few sharp peaks

✓ **Consistent attention range**
- Steady baseline with minor fluctuations
- Few red-marked unusual points

✓ **High confidence prediction**
- Usually 85-100% probability
- Clear "Real" bar dominance

### Fake Video Attention Patterns:

✗ **Erratic, irregular heatmaps**
- Off-diagonal attention clusters
- Sudden bright spots

✗ **Unstable self-attention curve**
- Sharp peaks and valleys
- Unusual patterns at specific frames

✗ **Volatile attention range**
- Sudden spikes and drops
- Multiple red-marked unusual points
- Evidence of frame-specific anomalies

✗ **Variable confidence prediction**
- May be lower confidence
- Clear "Fake" bar dominance (if detected as fake)

---

## Practical Analysis Steps

### Step 1: Check the Summary Visualization First
- Look at Plot 4 to see the prediction confidence
- If low confidence, all visualizations become less reliable

### Step 2: Examine the Attention Range (Plot 3)
- Count red-marked unusual frames
- These are the most suspicious points
- Note their frame numbers for manual inspection

### Step 3: Analyze Self-Attention Strength (Plot 2)
- Look for erratic behavior
- Identify frames with abnormally high or low values
- Compare suspicious frames with attention range plot

### Step 4: Study Per-Layer Heatmaps
- Focus on Layers 0 and 1 (early detection)
- Look for consistent vs inconsistent patterns across layers
- Identify which heads (1-8) show strongest anomalies

### Step 5: Create Hypotheses
- **If attention is concentrated on few frames**: Motion artifacts or static generation
- **If attention is highly local (diagonal only)**: Model relies on frame-level features
- **If attention spans entire video**: Complex temporal relationships matter
- **If patterns are irregular**: Strong deepfake signals

---

## Key Deepfake Indicators in Attention

| Pattern | Meaning | Confidence |
|---------|---------|-----------|
| Multiple red markers in attention range | Temporal inconsistencies detected | High |
| Sudden off-diagonal attention clusters | Unnatural frame relationships | High |
| Erratic self-attention curve with peaks | Problematic frames identified | High |
| Symmetric/regular attention patterns | Artificial or engineered sequence | Medium |
| Very low overall attention range | Frames are disconnected | High |
| All attention on early/late frames | Possible frame insertion/deletion | Medium |
| Uniform attention across all frames | Poor temporal understanding | Low |

---

## Common Patterns and What They Mean

### Pattern: **Diagonal Dominance**
- **Appearance**: Bright diagonal line, fading away
- **Real Video**: Expected - natural frame continuity
- **Fake Video**: If pure diagonal with NO off-diagonal attention, may indicate over-smoothing

### Pattern: **Horizontal Stripes**
- **Appearance**: Bright lines running left-right at specific frames
- **Meaning**: Those frames need information from many other frames
- **Deepfake Indicator**: If happening at unnatural points, suggests blending or artifact

### Pattern: **Vertical Stripes**
- **Appearance**: Bright lines running top-bottom at specific frames
- **Meaning**: Many frames need information from those key frames
- **Deepfake Indicator**: If concentrated at frame boundaries, suggests cuts/splices

### Pattern: **Checkerboard/Noise**
- **Appearance**: Random scattered bright spots
- **Meaning**: No clear temporal pattern
- **Interpretation**: Model is struggling, low confidence likely

---

## Advanced Analysis: Head-to-Head Comparison

Each transformer layer has 8 attention heads. Different heads may specialize:

- **Head 1**: Might focus on local continuity
- **Head 2**: Might capture long-range dependencies
- **Head 3**: Might detect periodic patterns
- **Head 4-8**: May handle different feature combinations

**Analysis Tip**: If one head shows strong anomalies while others are normal:
- That head may have detected specific artifact type
- Cross-reference with feature importance analysis

---

## Confidence Levels for Interpretation

| Attention Pattern | Confidence in Deepfake Detection |
|------------------|----------------------------------|
| Multiple strong indicators | Very High (>90%) |
| Clear unusual attention + erratic self-attn | High (75-90%) |
| Subtle patterns + moderate confidence | Medium (50-75%) |
| Ambiguous attention + low prediction confidence | Low (<50%) |

---

## Troubleshooting: What If Patterns Are Unclear?

**Problem**: All frames look similar in attention
- **Possible causes**: Generic features, low-resolution video, poor model fit
- **Next step**: Check feature importance analysis

**Problem**: Attention seems random/noisy
- **Possible causes**: Video too short, insufficient training data, model uncertainty
- **Next step**: Analyze with other explainability methods

**Problem**: Perfect diagonal, zero anomalies
- **Possible causes**: Over-smoothed fake, or truly natural video
- **Next step**: Check self-attention curve and prediction confidence

---

## Summary Checklist

✓ **To Interpret Transformer Attention**:
1. Check model prediction confidence (Plot 4)
2. Count unusual frames in attention range (Plot 3, red markers)
3. Look for erratic self-attention curve (Plot 2)
4. Examine per-layer heatmaps for consistency
5. Compare left/right halves of heatmaps (video structure)
6. Identify correlated anomalies across multiple visualizations
7. Validate findings against frame inspection

✓ **Red Flags for Deepfakes**:
- Multiple red-marked unusual frames
- Erratic self-attention pattern
- Inconsistent patterns across transformer layers
- Sudden off-diagonal attention clusters
- Asymmetric or non-smooth heatmaps

✓ **Green Flags for Real Videos**:
- Smooth, predictable attention patterns
- Stable self-attention curve
- Few unusual frames marked
- Consistent patterns across layers
- High prediction confidence
